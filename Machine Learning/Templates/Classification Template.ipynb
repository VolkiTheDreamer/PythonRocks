{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I highly recommend you get <a href=\"https://github.com/VolkiTheDreamer/PythonRocks/tree/master/mypyextj\">my custom package</a>(they are not pypi-installable yet), just download the folder via https://minhaskamal.github.io/DownGit/#/home onto your local.(read the readme file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mypyext import dataanalysis as da\n",
    "from dataprep.eda import plot, plot_correlation, plot_missing, create_report\n",
    "import sweetviz as sv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessors\n",
    "from mypyext import ml\n",
    "from sklearn.model_selection import train_test_split,cross_val_score,cross_val_predict,StratifiedKFold,RepeatedKFold,RepeatedStratifiedKFold\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer,IterativeImputer,KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler,RobustScaler\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, LabelEncoder\n",
    "from category_encoders import OrdinalEncoder as COE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.base import TransformerMixin,BaseEstimator\n",
    "from sklearn.pipeline import Pipeline,make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.model_selection import GridSearchCV,RandomizedSearchCV\n",
    "from sklearn.feature_selection import VarianceThreshold,SelectKBest, chi2, f_classif, mutual_info_classif,RFE,RFECV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier,GradientBoostingClassifier, AdaBoostClassifier,BaggingClassifier,VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "from sklearn.metrics import confusion_matrix,classification_report,plot_confusion_matrix,ConfusionMatrixDisplay\n",
    "from sklearn.metrics import auc,roc_auc_score,precision_recall_curve,roc_curve,brier_score_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.reaqd_csv(\"url\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, let's explore our dataset a little, let's see what's up and what's correlated with each other, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.super_info_() #my extension method, information about which can be found at https://mvolkanyurtseven.medium.com/top-n-useful-python-tips-tricks-e3a163e56749"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check <a href=\"https://mvolkanyurtseven.medium.com/top-n-useful-python-tips-tricks-e3a163e56749\">this link</a> in order to learn how to use extension methods and what they are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comments**\n",
    "\n",
    "- check for datatypes\n",
    "- comment on unnecessary columns\n",
    "- insert line in the pipeline for scaling if normal distribution assumption is sustained\n",
    "- insert line in the pipeline for discretizastion if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop columns that are of 1-cardinality, full-cardinality, and others you find necessary\n",
    "df.drop([],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the unique values of features with low <a href=\"https://en.wikipedia.org/wiki/Cardinality\">cardinality</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da.getColumnsInLowCardinality(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comments**\n",
    "\n",
    "- comment on ordinals\n",
    "- comment on numerics that are actually to be taken as categoric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's determine on our feature types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target=[]\n",
    "nums=[]\n",
    "cats=list(df.columns).removeItemsFromList_(nums+target,False) #extension method\n",
    "ords=[]\n",
    "noms=cats.removeItemsFromList_(ords,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "insert line in the pipeline for encoding(ordinal+onehot) operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Visuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df[nums],height=1, aspect=1.2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comments**\n",
    "\n",
    "- diagonal histogram for skewness, multicollinearty?\n",
    "- insert line in the pipeline for log transformation if necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the correlations closer. We will use the <a href=\"https://github.com/shakedzy/dython\">dython</a> library for this. Because, with this library, both numeric-numeric, numeric-categorical and categorical-categorical correlations can be obtained with a single function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dython.nominal import associations\n",
    "corrdict=associations(df,nominal_columns=cats,numerical_columns=nums,figsize=(10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the top N features that correlate with target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_results=corrdict[\"corr\"] #dataframe\n",
    "da.getHighestPairsOfCorrelation(corr_results,target?,N?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use these correlation values in the feature selection stage. By the way, we said in the above charts that x,y etc. would be of high importance. Indeed, they are also highly correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[nums].plot(kind=\"box\", subplots = True,figsize=(8,5))\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "for e,n in enumerate(nums):\n",
    "    plt.subplot(1,len(nums),e+1) \n",
    "    ch=sns.violinplot(y=n, data=df)\n",
    "plt.tight_layout()    \n",
    "plt.show();    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da.outlierinfo(df,nums,imputestrategy=\"None\",thresh=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da.outliers_IQR(df,nums,imputestrategy=\"None\",thresh=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da.outliers_IQR(df,nums,imputestrategy=\"None\",thresh=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**comments**\n",
    "\n",
    "- .....\n",
    "- insert line in the pipeline for outlier-handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for Nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno as msno\n",
    "msno.bar(df, figsize=(8,5));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da.nullPlot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check for null-like values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T18:43:52.083311Z",
     "start_time": "2022-02-04T18:43:50.072076Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are no null-like values\n"
     ]
    }
   ],
   "source": [
    "da.findNullLikeValues(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**comments**\n",
    "\n",
    "- .....\n",
    "- insert line  in the pipeline for null-handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target-based analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take first a look at the number of instances in differenta values of the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=df[\"targetcolumn\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "insert line in the pipeline for oversapling/undersampling in case of imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now take a look at the average values of numeric features on the basis of target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da.plotNumericsByTarget(df,\"target\",nums=nums,layout=(1,5),figsize=(12, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comments**\n",
    "\n",
    "- Outliers can mislead the interpretation of the results, if any.\n",
    "- feature importance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the categoric features, lets check the probabilities of each value of the feature on the positive target.??????????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-24T09:01:15.464086Z",
     "start_time": "2022-05-24T09:01:15.444720Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,8))\n",
    "da.plotTargetByCats(df, cats, \"target\", subplot_tpl=(2,4));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comments**\n",
    "\n",
    "- ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, lets look at the distribution of the positive target on the categoric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,8))\n",
    "da.plotPositiveTargetByCats(df, cats, \"target\", subplot_tpl=(2,4));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comments**\n",
    "\n",
    "Most of the \"positive class\" are those with:\n",
    "\n",
    "- ...\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-15T16:06:40.964670Z",
     "start_time": "2022-05-15T16:06:40.940488Z"
    }
   },
   "outputs": [],
   "source": [
    "# da.plotTargetForNumCatsPairs(dfheart2,nums,cats,\"AHD\",2.4,0.9)\n",
    "# da.plotCategoricForNumTargetPairs(dfheart2,nums,cats,\"AHD\",2.4,0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numeric borders, check the min-max\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check unique values in low-cardinalty features again to see if there are any abnormal values that're not supposed to be there"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#duplicate check for rows\n",
    "len(df)-len(df.duplicated(keep=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#duplicate check for columns\n",
    "len(set(df.columns))-len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multicollinearty check->remove one if r>0.9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data quality to peculiar to this specific data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# any need for conversion, 100 USD-->100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for feature extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "insert line in the pipeline for necessary processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking for assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whatever algorithm you will use, check its assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing X,y and train-test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.iloc[:,:-1]\n",
    "y=df.iloc[:,-1].values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if imbalanced add stratify=y\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(map(np.shape, (X_train, X_test, y_train, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(map(type, (X_train, X_test, y_train, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If necessary, crete your custom function and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "def mycustomFunc(X):\n",
    "    return X #or X.values is to be passed dataframe\n",
    "    \n",
    "class OutlierHandler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, featureindices): #if only specific columns to be processed\n",
    "        self.featureindices = featureindices\n",
    "  \n",
    "    def fit(self, X:np.array, y = None):\n",
    "        Q1s = np.quantile(X[:,self.featureindices],0.25,axis=0)\n",
    "        Q3s = np.quantile(X[:,self.featureindices],0.75,axis=0)\n",
    "        IQRs = Q3s-Q1s\n",
    "        self.top=(Q3s + 1.5 * IQRs)\n",
    "        self.bottom=(Q1s - 1.5 * IQRs)\n",
    "        return self \n",
    "    \n",
    "    def transform(self, X:np.array, y = None ):\n",
    "        X[:,self.featureindices]=np.where(X[:,self.featureindices]>self.top,self.top,X[:,self.featureindices])\n",
    "        X[:,self.featureindices]=np.where(X[:,self.featureindices]<self.bottom,self.bottom,X[:,self.featureindices])\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build our pipeline, this is only a template, we need to enhance it. First we can only try out numeric-ranged parametres in a very large space, and then narrow it on the second run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_range=np.logspace(4,-5, num=10)\n",
    "weights = np.arange(2,11,2)\n",
    "min_res=10\n",
    "fact=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingRandomSearchCV,RandomizedSearchCV\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "class DummyTransformer(TransformerMixin,BaseEstimator):\n",
    "    def fit(self,X,y=None): pass\n",
    "    def transform(self,X,y=None): pass\n",
    "    \n",
    "class DummyEstimator(BaseEstimator):\n",
    "    def fit(self,X,y=None): pass\n",
    "    def score(self,X,y=None): pass    \n",
    "    \n",
    "cat_pipe=Pipeline([ \n",
    "                   (\"csi\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                   (\"ohe\", OneHotEncoder(drop=\"first\",handle_unknown='ignore'))     \n",
    "                  ])\n",
    "\n",
    "num_pipe=Pipeline([  \n",
    "                   (\"nsi\", FunctionTransformer(numericImputer)), \n",
    "                   (\"ouh\", DummyTransformer()), \n",
    "                   (\"scl\", DummyTransformer())\n",
    "                  ])\n",
    "\n",
    "coltrans = ColumnTransformer([\n",
    "                                ('nominals',  cat_pipe, noms),\n",
    "                                ('ordinals',  OrdinalEncoder(categories=[[]]), [\"ChestPain\"]),\n",
    "                                ('numerics',  num_pipe, nums)\n",
    "                                 ],n_jobs=-1,remainder=\"passthrough\")\n",
    "\n",
    "pipe = Pipeline(steps=[('ct', coltrans),\n",
    "                       ('fs', SelectKBest(score_func=mutual_info_classif,k=10)), \n",
    "                       ('clf', DummyEstimator()) \n",
    "                       ])\n",
    "\n",
    "params = [          \n",
    "          {\n",
    "           'clf'         : [LogisticRegression(max_iter=1000,random_state=42)],\n",
    "           'clf__C'      : c_range, \n",
    "           'clf__penalty': ['l2'], \n",
    "           'clf__solver' : ['newton-cg', 'lbfgs'],\n",
    "           'clf__class_weight': [{\"No\":1, \"Yes\":x} for x in weights] + ['balanced']                \n",
    "           'ct__numerics__ouh': [OutlierHandler(featureindices=[1]),None],\n",
    "           'ct__numerics__scl': [StandardScaler(),RobustScaler()]\n",
    "          } ,\n",
    "    \n",
    "          {\n",
    "           'clf'         : [DecisionTreeClassifier(random_state=42)],\n",
    "           'clf__criterion': ['gini', 'entropy'],\n",
    "           'clf__max_depth': [2,3],\n",
    "           'clf__min_samples_split':[2,4],\n",
    "           'clf__class_weight': [{\"No\":1, \"Yes\":x} for x in weights] + ['balanced']                \n",
    "           'ct__numerics__ouh': [None],\n",
    "           'ct__numerics__scl': [None]\n",
    "          }  \n",
    "         ]         \n",
    "\n",
    "mycv = RepeatedKFold(n_splits=5, n_repeats=10, random_state=1)\n",
    "hrs1 = HalvingRandomSearchCV(estimator = pipe, param_distributions = params, cv = mycv, n_jobs=-1, verbose = 1, \n",
    "                           scoring = 'accuracy',error_score='raise',min_resources=min_res,factor=fact) \n",
    "\n",
    "hrs1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new c_range\n",
    "c_range=[]\n",
    "\n",
    "params = [          \n",
    "          {\n",
    "           'clf'         : [LogisticRegression(max_iter=1000,random_state=42)],\n",
    "           'clf__C'      : c_range2, \n",
    "           'clf__penalty': ['l2'], \n",
    "           'clf__solver' : ['newton-cg', 'lbfgs'],\n",
    "           'clf__class_weight': [{\"No\":1, \"Yes\":x} for x in weights] + ['balanced']              \n",
    "           'ct__numerics__ouh': [OutlierHandler(featureindices=[1]),None],\n",
    "           'ct__numerics__scl': [StandardScaler(),RobustScaler()]\n",
    "          } ,\n",
    "    \n",
    "          {\n",
    "           'clf'         : [DecisionTreeClassifier(random_state=42)],\n",
    "           'clf__criterion': ['gini', 'entropy'],\n",
    "           'clf__max_depth': [2,3],\n",
    "           'clf__min_samples_split':[2,4],\n",
    "           'clf__class_weight': [{\"No\":1, \"Yes\":x} for x in weights] + ['balanced'] \n",
    "           'ct__numerics__ouh': [None],\n",
    "           'ct__numerics__scl': [None]\n",
    "          }  \n",
    "         ]    \n",
    "\n",
    "hrs = HalvingRandomSearchCV(estimator = pipe, param_distributions = params, cv = mycv, n_jobs=-1, verbose = 1, \n",
    "                           scoring = 'accuracy',error_score='raise',min_resources=60,factor=3) \n",
    "\n",
    "hrs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml.gridsearch_to_df(hrs2)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml.compareEstimatorsInGridSearch(gs4,tableorplot='table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml.compareEstimatorsInGridSearch(gs4,tableorplot='plot',figsize=(4,4))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-Check for assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your algorithms require post-check, do it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml.plot_learning_curve(best_estimator_,\"Learnig curve\",X_train,y_train,cv=mycv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T18:00:00.860428Z",
     "start_time": "2022-06-08T18:00:00.848458Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "ml.plot_confusion_matrix(cm,classes=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if not imbalanced\n",
    "ml.plotROC(y_test, X_test, gs4, pos_label=\"Yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml.plot_precision_recall_curve(y_test_le,X_test,gs4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml.plot_gain_and_lift(gs4,X_test,y_test,pos_label=\"Yes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's run the gridsearch with log loss optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-21T17:32:55.752736Z",
     "start_time": "2022-05-21T17:32:53.590829Z"
    }
   },
   "outputs": [],
   "source": [
    "hrs = HalvingRandomSearchCV(estimator = pipe, param_distributions = params, cv = mycv, n_jobs=-1, verbose = 1, \n",
    "                           scoring = 'neg_log_loss',error_score='raise',min_resources=60,factor=3) \n",
    "\n",
    "hrs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-21T17:33:02.443968Z",
     "start_time": "2022-05-21T17:33:02.403084Z"
    }
   },
   "outputs": [],
   "source": [
    "ml.gridsearch_to_df(hrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml.find_best_cutoff_for_classification(hrs, y_test_le, X_test,[0,5,100,60])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if lenar model\n",
    "ml.linear_model_feature_importance(gs4,coltrans,\"fs\",\"clf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if tree based\n",
    "#ml.get_feature_names_from_columntransformer\n",
    "ml.eatureImportanceEncoded(feature_importance_array,feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "383.963px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
