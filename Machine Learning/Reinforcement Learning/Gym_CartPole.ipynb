{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VolkiTheDreamer/PythonRocks/blob/master/Machine%20Learning/Reinforcement%20Learning/Gym_CartPole.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-QtuMh74EBB"
      },
      "source": [
        "# Starters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9E-Ztr54QMF"
      },
      "outputs": [],
      "source": [
        "!pip -q install ipython-autotime\n",
        "%load_ext autotime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixKvhpkb4QMH"
      },
      "outputs": [],
      "source": [
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJ_so_Yk3zH0"
      },
      "source": [
        "# Sıfırda manuel oluşturma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CON5dE1rf43N"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import random\n",
        "import pickle\n",
        "import time\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzwKkCvq33Dc"
      },
      "source": [
        "(Medium yazısına link, after publish)\n",
        "\n",
        "Environemnet hakkındaki detay bilgilere [buradan](https://gymnasium.farama.org/environments/classic_control/cart_pole/) ulaşabilrisiniz.\n",
        "\n",
        "Ben tensorflow kullancağım. Torch ile örnek uygulama göremk isterseniz\n",
        "[burada](https://github.com/ritakurban/Practical-Data-Science/blob/master/DQL_CartPole.ipynb) detaylı bir anlatım sözkonusu, bakılabilir veya aşağıdaki benim kodları kendiniz torch'a çevirmek isteyebilirsiniz, bunla da uğraşmak istemezseniz ChatGPT veya benzeri bir araca dönüşüm sağlatabilirsiniz. Ancak ben sıfırdan sizin manuel yazmanızı tavsiye ederim.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvZwWnSzP87C"
      },
      "source": [
        "## Ortak Kullanılacak Train ve Test fonksiyonları"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxE1jiE8QCV4"
      },
      "source": [
        "Bu fonksiyonları tüm yöntemlerde kullanacağız, o yüzden en başa aldım ama bu kısmı şimdi atlayıp öncelikle Pure DQL kısmına gelin. Orayı inceledikten sonra bunlara tekrar bakarsınız."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Öncelikle klasik Q-Learningde kurguladığımız eğitim kodunu kodlayalım. Dıştaki for döngümüz episodelar için olacak, içteki while döngümüz ise ilgili episode bitene kadar sürekli devam edecek."
      ],
      "metadata": {
        "id": "2qQbeip4fUHS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddsfKfIToBCn"
      },
      "outputs": [],
      "source": [
        "def train_model(env, agent, episodes = 100, print_episode=True):\n",
        "  scores = []\n",
        "  for e in tqdm(range(episodes)):\n",
        "      state,_ = env.reset()\n",
        "      if printInsideTraining:\n",
        "        print(f\"state before reshape: {state}, shape: {state.shape}\")\n",
        "\n",
        "      state = state.reshape(1,-1)\n",
        "      if printInsideTraining:\n",
        "        print(f\"state after reshape: {state}, shape: {state.shape}\")\n",
        "\n",
        "      sayac = 0\n",
        "      while True:\n",
        "          if printInsideTraining:\n",
        "            print(f\"\\nsayac:{sayac}\")\n",
        "          if agent is None:\n",
        "            action = env.action_space.sample()\n",
        "          else:\n",
        "            action = agent.choose_action(state)\n",
        "          if printInsideTraining:\n",
        "              print(f\"seçilen action:{action}\")\n",
        "          # bu action'a göre hareket edelim\n",
        "          next_state, reward, terminated, truncated , _= env.step(action) #son parametre info olup gereksizdir\n",
        "          done = terminated or truncated\n",
        "          if printInsideTraining:\n",
        "              print(f\"nextstate before reshape:{next_state} ve shape'i: {next_state.shape}\")\n",
        "\n",
        "          next_state = next_state.reshape(1,-1)\n",
        "          if printInsideTraining:\n",
        "              print(f\"nextstate after reshape:{next_state} ve shape'i: {next_state.shape}\")\n",
        "          if agent is not None:\n",
        "            # yeni edindiğimiz deneyimi hafızaya atalım\n",
        "            agent.add_experince_to_memory(state, action, reward, next_state, done)\n",
        "          # state'i güncelleyelim\n",
        "          state = next_state\n",
        "\n",
        "          if agent is not None:\n",
        "            # eğitimi tamamlayalım\n",
        "            agent.replay_vectorized() #T4 GPU'da 100 episodes 13 dk, vectorize olmayan metod 9 saat sürdü\n",
        "          if agent is not None:\n",
        "            # epsilon'ı güncelleyelim\n",
        "            if agent.epsilon > agent.epsilon_min:\n",
        "                agent.epsilon *= agent.epsilon_decay\n",
        "          #sayacı 1 artıralım\n",
        "          sayac += 1\n",
        "          if done:\n",
        "              if print_episode:\n",
        "                print(f\"Episode: {e}, sayaç: {sayac}\")\n",
        "              scores.append(sayac)\n",
        "              break\n",
        "  plt.plot(scores)\n",
        "  plt.axhline(np.mean(scores), color='red', linestyle='dashed')\n",
        "  _, max_xlim = plt.xlim()\n",
        "  plt.text(max_xlim*1.01, np.mean(scores)*0.99, f'Ortalama: {np.mean(scores):.1f}')\n",
        "  plt.text(np.argmax(scores)*1.01, np.max(scores)*0.99, f'Max: {np.max(scores):.1f}')\n",
        "\n",
        "  thresh = 500\n",
        "  plt.axhline(thresh, color='blue')\n",
        "  plt.text(max_xlim*1.01, thresh*0.99, f'Kazanma eşik noktası:{thresh}')\n",
        "  plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFlXnp_FsaT4"
      },
      "outputs": [],
      "source": [
        "from gymnasium.wrappers import RecordEpisodeStatistics, RecordVideo\n",
        "def test_model(env, trained_agent, n):\n",
        "  #n kez oynasın\n",
        "\n",
        "  scores=[]\n",
        "  for i in tqdm(range(n)):\n",
        "    state ,_= env.reset()\n",
        "    state = state.reshape(1,-1)\n",
        "    sayac = 0\n",
        "    while True:\n",
        "        if trained_agent is not None:\n",
        "          action = trained_agent.choose_action(state)\n",
        "        else:\n",
        "          action = env.action_space.sample()\n",
        "        next_state, reward, terminated, truncated ,_= env.step(action)\n",
        "        done = terminated or truncated\n",
        "        next_state = next_state.reshape(1,-1)\n",
        "        state = next_state\n",
        "        sayac += 1\n",
        "        if done:\n",
        "            break\n",
        "    print(f\"{i}. oyunda {sayac} hamle dayandı\")\n",
        "    scores.append(sayac)\n",
        "\n",
        "  print(f\"Ortalama dayanma süresi:{np.mean(scores)} hamle\")\n",
        "  print(f'Episode time taken: {env.time_queue}')\n",
        "  print(f'Episode total rewards: {env.return_queue}')\n",
        "  print(f'Episode lengths: {env.length_queue}')\n",
        "  env.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import base64\n",
        "from IPython.display import HTML\n",
        "import moviepy.editor as mpy\n",
        "\n",
        "def show_video(video_path, slow_factor=None):\n",
        "  if slow_factor is not None:\n",
        "    clip = mpy.VideoFileClip(video_path)\n",
        "    slow_motion_clip = clip.fx(mpy.vfx.speedx, 1/slow_factor)\n",
        "    slow_motion_clip.write_videofile(f\"{video_path[:-4]}_sw.mp4\")\n",
        "    video = open(f\"{video_path[:-4]}_sw.mp4\", 'rb').read()\n",
        "  else:\n",
        "    video = open(video_path, 'rb').read()\n",
        "  encoded = base64.b64encode(video)\n",
        "  return HTML(data='''<video alt=\"test\" autoplay\n",
        "                controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii')))\n",
        "\n",
        "def save_as_gif(video_path):\n",
        "  clip = mpy.VideoFileClip(video_path)\n",
        "  clip.write_gif(f\"{video_path[:-4]}.gif\")"
      ],
      "metadata": {
        "id": "59ktfnzonOZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYGvxcJjhih7"
      },
      "source": [
        "## Random action stratejisi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6gKR_xqhh3o"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"CartPole-v1\") #env'i yaratalım\n",
        "printInsideTraining=False\n",
        "train_model(env, agent = None, episodes=100, print_episode=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDz31P6isEky"
      },
      "outputs": [],
      "source": [
        "# test\n",
        "num_eval_episodes = 10\n",
        "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "env = RecordVideo(env, video_folder=\"cartpole-agent\", name_prefix=\"random-test\",\n",
        "                  episode_trigger=lambda x: True)\n",
        "env = RecordEpisodeStatistics(env, buffer_length=num_eval_episodes)\n",
        "test_model(env, None, n=num_eval_episodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdthXt1ixh4K"
      },
      "outputs": [],
      "source": [
        "show_video('cartpole-agent/random-test-episode-0.mp4', slow_factor=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ozspq3QRMEt"
      },
      "source": [
        "## Pure DQL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mQBrCseRP9T"
      },
      "outputs": [],
      "source": [
        "class DQLAgent:\n",
        "    def __init__(self, env, loss=\"mse\",batch_size=16,learning_rate=0.001,memory_size=1000):\n",
        "        # bu class'tan bir obje yaratıldığında set edilecek değerler ve çalıştırılacak metodlar\n",
        "        self.state_size = env.observation_space.shape[0] #4\n",
        "        self.action_size = env.action_space.n #2\n",
        "        self.gamma = 0.95\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epsilon = 1  # explore\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.epsilon_min = 0.01\n",
        "        self.memory = deque(maxlen = memory_size) #replay meomry hacmi\n",
        "        self.verbose = False\n",
        "        self.batch_size= batch_size\n",
        "\n",
        "        self.online_model = self.build_model(loss) # sadece Q networkünü oluşturuyoruz\n",
        "\n",
        "    def save(self, file):\n",
        "        pickle.dump(self, open(file, \"wb\"))\n",
        "\n",
        "    def build_model(self, loss):\n",
        "        model = Sequential()\n",
        "        model.add(Dense(24, input_dim = self.state_size, activation = \"relu\")) #Ideal senaryoda node adedi tune edilebilir, biz şuan buna takılmayacağız, 48 sabit kalsın\n",
        "        model.add(Dense(48, input_dim = self.state_size, activation = \"relu\")) #bu araya tercihe göre bir katman daha eklenebilir\n",
        "        model.add(Dense(self.action_size, activation = \"linear\")) #output layer'ımızda action adedi kadar node olacak\n",
        "        model.compile(loss = loss, optimizer = Adam(learning_rate = self.learning_rate))\n",
        "        return model\n",
        "\n",
        "    def add_experince_to_memory(self, state, action, reward, next_state, done):\n",
        "        # deneyimleri hafızaya atıyoruz, sonra buradan random kayıt seçceğiz\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "        if self.verbose:\n",
        "            print(f\"replay memory hacmi:{len(self.memory)}\")\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        # acting: explore or exploit\n",
        "        if random.uniform(0,1) <= self.epsilon:\n",
        "            return env.action_space.sample()\n",
        "        else:\n",
        "            act_values = self.online_model.predict(state, verbose=0)\n",
        "            return np.argmax(act_values[0])\n",
        "\n",
        "    def _replay(self): #*********************************************************DİKKAT: Kullanılmayacak, alttaki açıklamayı okuyun******************************************************\n",
        "        # öğrenemyi sağlayan metoddur. adını train veya learn olarak da değiştirebiliriz\n",
        "        # birçok blogda/eğitimde replay metodu bu şekilde kodlanır ama buradaki for döngüsü çalışma süresi açısından efektif değildir, biz bi alttaki vektörize halini kullanacağız\n",
        "        # rakamsal örnek vermek gerekirse, T4 FPU'da, 100 episode için çalışması 9 saat sürerken, vectorized olan 15 dk sürmüştür\n",
        "        if len(self.memory) < self.batch_size: #agent'ın hafızasında yeterince deneyim yoksa devam etmeyelim, deneyim biriktirmeye devam edelim\n",
        "            return\n",
        "        minibatch = random.sample(self.memory,self.batch_size)\n",
        "        for state, action, reward, next_state, done in minibatch:\n",
        "            if done:\n",
        "                target = reward\n",
        "            else:\n",
        "                target = reward + self.gamma*np.amax(self.online_model.predict(next_state, verbose=0)[0])\n",
        "            train_target = self.online_model.predict(state, verbose=0)\n",
        "            train_target[0][action] = target\n",
        "            #1 epoch ile eğitilmesinin nedeni, online öğrenme, kararlılık, verimlilik ve deneyim tekrarı tekniklerinin etkinliğini en üst düzeye çıkarmaktır.\n",
        "            #Bu, modelin yeni bilgilere hızlı bir şekilde uyum sağlamasına, öğrenme sürecini stabilize etmesine, eğitim süresini azaltmasına ve aşırı uyum sağlamasını önlemeye yardımcı olur.\n",
        "            self.online_model.fit(state,train_target, epochs=1, verbose = 0)\n",
        "\n",
        "    def replay_vectorized(self):\n",
        "        # üstteki metodun vektörize halidir\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return\n",
        "        minibatch = random.sample(self.memory,self.batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*minibatch) # [(1, 2, 3, 4, 5), (10, 20, 30, 40, 50)] gibi bir listeyi [(1, 10,.....), (2, 20,......), (3, 30,.......), (4, 40,.......), (5, 50,......)] haline çeviriyoruz, ki bunları numpy diziye çevirebilelim\n",
        "        if self.verbose:\n",
        "          print(f\"states before numpy dönüşüm in replay metodu:{states}\")\n",
        "        # numpy array'e çevrielim\n",
        "        self.states = np.array(states)\n",
        "        self.actions = np.array(actions)\n",
        "        self.rewards = np.array(rewards)\n",
        "        self.next_states = np.array(next_states)\n",
        "        self.dones = np.array(dones)\n",
        "\n",
        "        if self.verbose:\n",
        "          print(f\"states before reshape in replay:{self.states}, states.shape:{self.states.shape}\")\n",
        "        self.states = self.states.reshape(self.batch_size, self.state_size)  #  (16,1,4) olan shape'i (16,4) yapıyoruz\n",
        "\n",
        "        if self.verbose:\n",
        "          print(f\"states after reshape in replay:{self.states},states.shape:{self.states.shape}\")\n",
        "\n",
        "        self.next_states = self.next_states.reshape(self.batch_size, self.state_size)\n",
        "        self.do_calculation() #normalde bunu ayrı metodu yapmaya gerek yoktu ama üst kısmı başka yerde de kullanacağımız için modüler hale getiriyorum\n",
        "\n",
        "    def do_calculation(self):\n",
        "        q_expected = self.online_model.predict(self.states, verbose=0) #mevcut statelerden Q-valueları tahminleme yapalım, Q(s,a)\n",
        "        if self.verbose:\n",
        "          print(f\"q_expected:{q_expected}\")\n",
        "\n",
        "        q_next_target = self.online_model.predict(self.next_states, verbose=0)  # next statelerden Q(s',a') tahminleyelim.\n",
        "        if self.verbose:\n",
        "          print(f\"q_next:{q_next_target}\")\n",
        "\n",
        "        max_next_q = np.max(q_next_target, axis=1) # q_next_target içindeki en yüksek Q-value'larını alalım\n",
        "        if self.verbose:\n",
        "          print(f\"max_next_q:{max_next_q}\")\n",
        "\n",
        "        q_targets = self.rewards + self.gamma * max_next_q * (1 - self.dones) #target Q-values hesabı\n",
        "        if self.verbose:\n",
        "          print(f\"q_targets:{q_targets}\")\n",
        "\n",
        "        # Hangi aksiyon seçildiyse q_expectedda bu aksiyonu targetta ki ile update edelim\n",
        "        # for idx, action in enumerate(actions):\n",
        "        #     q_expected[idx][action] = q_targets[idx]\n",
        "        q_expected[np.arange(self.batch_size), self.actions] = q_targets #vectorized olarak\n",
        "        if self.verbose:\n",
        "          print(f\"q_expected after update:{q_expected}\")\n",
        "\n",
        "        # eğitelim\n",
        "        self.online_model.fit(self.states, q_expected, epochs=1, verbose=0)\n",
        "        self.verbose = False #sadece bir kez bastırmak yeterli bence. Tümünü bastırmak isteyen commentleyebilir\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Şimdi agentımızı yaratıp eğitelim."
      ],
      "metadata": {
        "id": "3I3dU1frfeMA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLv4vrlXRYej"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"CartPole-v1\") #env'i yaratalım\n",
        "dqlagent = DQLAgent(env) #agent objesini yaratalım, init metodu tetiklenecek\n",
        "# agent.verbose = True #Uncomment yaparak repaly metodu içindeki print'li kısımların görünmesini sağlayabilirsiniz\n",
        "printInsideTraining = False # T/F yaparak bu hücre içindeki printlerin görünüp görünmemesini sağlayabilirsiniz\n",
        "# kodda kalabalık odluğunu düşünüyorsanız ve print'lere gerek duymuyorsanız o satırları silebilirsiniz\n",
        "\n",
        "train_model(env, dqlagent, episodes=200, print_episode=False) #ilk 100 civarını hızlı öğreniyor ama yeterli öğrenme olmadığı için hızlı oluyr, sonra yavaş yaval güzel öğreniyor, daha uzun sürüyor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7gdmofn5njy"
      },
      "source": [
        "110-120'den sonra güzel öğrenmiş gibi. yüksek skorlara ulaşıp sonra tekrar düşmesinin en büyük sebebi, ar ara keşif yapmaya devam etmesi olarak düşünülebilir."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eyt3SyPVQmEf"
      },
      "outputs": [],
      "source": [
        "#agent'ımızı sonraki kullanımları için kaydedelim\n",
        "dqlagent.save(\"dqlagent.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYY2pDCgRYSl"
      },
      "outputs": [],
      "source": [
        "# test\n",
        "num_eval_episodes = 10\n",
        "env_test = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "env_test = RecordVideo(env_test, video_folder=\"cartpole-agent\", name_prefix=\"dql-test\",\n",
        "                  episode_trigger=lambda x: True)\n",
        "env_test = RecordEpisodeStatistics(env_test, buffer_length=num_eval_episodes)\n",
        "trained_agent = pickle.load(open(\"dqlagent.pkl\", \"rb\"))\n",
        "test_model(env_test, trained_agent, n=num_eval_episodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAaixxBu1Lsx"
      },
      "outputs": [],
      "source": [
        "show_video('cartpole-agent/dql-test-episode-0.mp4')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0y217G-bHSO"
      },
      "source": [
        "## DQN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCOYR9ACFp1I"
      },
      "source": [
        "Şimdi DQNAgent adında bir class yaratacağız. Bu, birçok açıdan DQLAgent'a benzeyecek, herşeyi tekrar tekrar kodlamak yerine DQLAgent'ı inherit eden bir class yaratalım ve sadece replay metodunu override edelim."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RvBzq82AGwb6"
      },
      "outputs": [],
      "source": [
        "class DQNAgent(DQLAgent):\n",
        "    def __init__(self, env, loss=\"mse\", batch_size=16, learning_rate=0.001, memory_size=1000, target_update=50):\n",
        "        super().__init__(env, loss, batch_size, learning_rate, memory_size)\n",
        "        #DQLAgent'a ek olarak şunları yapalım\n",
        "\n",
        "        # modelleri oluşturalım\n",
        "        self.online_model   = self.build_model(loss)\n",
        "        self.target_model   = self.build_model(loss)\n",
        "        self.target_model.set_weights(self.online_model.get_weights())  # Başlangıçta ağırlıkları eşitleyelim\n",
        "        self.target_update  = target_update #her 50 updatede güncelleme oalcak, tune edilebilir. çok yüksek seçilirse, hiç update edemeden oyun sonlanabilir, küçük sayılar seçilmeli\n",
        "        self.update_counter = 0\n",
        "\n",
        "\n",
        "    def do_calculation(self):\n",
        "        # 1) Target Q’ları hesapla (target newotrk üzerinden)\n",
        "        q_next_target = self.target_model.predict(self.next_states, verbose=0)   # Target ağdan next Q’ları al\n",
        "        max_next_q    = np.max(q_next_target, axis=1)                       # en büyük değeri seç\n",
        "        q_targets      = self.rewards + self.gamma * max_next_q * (1 - self.dones)\n",
        "        if self.verbose:\n",
        "          print(f\"q_next_target:{q_next_target}\")\n",
        "\n",
        "        self.continue_calculation(q_targets) #bu kısmı aynen double DQN'de de kullanacağımız için ayırdık, sadece üst kısım değişecek.\n",
        "\n",
        "\n",
        "    def continue_calculation(self,q_targets):\n",
        "        # 2) Online ağdan mevcut Q tahminlerini al\n",
        "        q_expected = self.online_model.predict(self.states, verbose=0)\n",
        "\n",
        "        # 3) Sadece seçilen aksiyonun Q’sunu güncelle\n",
        "        q_expected[np.arange(self.batch_size), self.actions] = q_targets\n",
        "\n",
        "        # 4) Bu batch’ten öğren\n",
        "        self.online_model.fit(self.states, q_expected, epochs=1, verbose=0)\n",
        "\n",
        "        # 5) Target ağı periyodik güncelle\n",
        "        self.update_counter += 1\n",
        "        if self.update_counter % self.target_update == 0:\n",
        "            self.target_model.set_weights(self.online_model.get_weights())\n",
        "            if self.verbose:\n",
        "              print(f\"Updating target network at step {self.update_counter}\")\n",
        "\n",
        "        self.verbose = False #sadece bir kez bastırmak yeterli bence. Tümünü bastırmak isteyen commentleyebilir\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7LAjhCooGyD"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"CartPole-v1\")\n",
        "dqnagent = DQNAgent(env) #DQL'den farklı olarak sadece burayı değiştriyirouz\n",
        "# agent.verbose = True #Uncomment yaparak repaly metodu içindeki print'li kısımların görünmesini sağlayabilirsiniz\n",
        "printInsideTraining = False # T/F yaparak bu hücre içindeki printlerin görünüp görünmemesini sağlayabilirsiniz\n",
        "# kodda kalabalık odluğunu düşünüyorsanız ve print'lere gerek duymuyorsanız o satırları silebilirsiniz\n",
        "\n",
        "train_model(env, dqnagent,episodes=200, print_episode=False) #ilk 100 civarında çok hızlı ilerledi, 15 dk gibi, sonra hızlı öğrenmeye başladı ve eğitim süresi uzadı"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcg326TuZPuX"
      },
      "outputs": [],
      "source": [
        "dqnagent.save(\"dqnagent.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LcaRXOaozZj"
      },
      "outputs": [],
      "source": [
        "# test\n",
        "num_eval_episodes = 10\n",
        "env_test = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "env_test = RecordVideo(env_test, video_folder=\"cartpole-agent\", name_prefix=\"dqn-test\",\n",
        "                  episode_trigger=lambda x: True)\n",
        "env_test = RecordEpisodeStatistics(env_test, buffer_length=num_eval_episodes)\n",
        "trained_agent = pickle.load(open(\"dqnagent.pkl\", \"rb\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_model(env_test, trained_agent, n=num_eval_episodes)"
      ],
      "metadata": {
        "id": "ty5exl_fgUbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AyfNUNc_5gnd"
      },
      "outputs": [],
      "source": [
        "show_video('cartpole-agent/dqn-test-episode-0.mp4')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMZJ4MoupdXO"
      },
      "source": [
        "### Hyperparameter tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tüm hyperparamtreleri değil de sadece birkaçı için deneme yapacağım, Bunların dışında\n",
        "- loss olarak huber denedim ve başarsı düşük çıktığı için burada ele almıyorum\n",
        "- Belki bir de optimizasyon yöntemi olarak farklı bişey denenebilir\n",
        "- Activation olarak da tanh ve leaky relu da denenebilir.\n",
        "\n",
        "ancak eğitim süresini çok daha fazla artırmak istemediğim için bunları yapmıyorum."
      ],
      "metadata": {
        "id": "5Pm8HyHalc0z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import ParameterGrid\n",
        "param_grid = {'update': [5,10,20,40,80],\n",
        "              \"batch_size\":[32,64],\n",
        "              \"lr\":[0.0001,0.005],\n",
        "              \"memory_size\":[2000,5000,10000]\n",
        "              }"
      ],
      "metadata": {
        "id": "H2q4VRdzjVxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for e,params in enumerate(ParameterGrid(param_grid)):\n",
        "  print(params)\n",
        "  env = gym.make(\"CartPole-v1\")\n",
        "  dqnagent = DQNAgent(env,\"mse\",params[\"batch_size\"],params[\"lr\"],params[\"memory_size\"],params[\"update\"])\n",
        "  printInsideTraining = False # T/F yaparak bu hücre içindeki printlerin görünüp görünmemesini sağlayabilirsiniz\n",
        "  train_model(env, dqnagent, episodes=150, print_episode=False) #ilk 100 civarında çok hızlı ilerledi, 15 dk gibi, sonra hızlı öğrenmeye başladı ve eğitim süresi uzadı\n",
        "  dqnagent.save(f\"dqnagent_param_{e}.pkl\")"
      ],
      "metadata": {
        "id": "F3cvEEJojVlU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test\n",
        "num_eval_episodes = 10\n",
        "env_test = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "env_test = RecordVideo(env_test, video_folder=\"cartpole-agent\", name_prefix=\"dqn-test-bestparam\",\n",
        "                  episode_trigger=lambda x: True)\n",
        "env_test = RecordEpisodeStatistics(env_test, buffer_length=num_eval_episodes)\n",
        "trained_agent = pickle.load(open(\"dqnagent_param_16.pkl\", \"rb\"))\n",
        "test_model(env_test, trained_agent, n=num_eval_episodes)"
      ],
      "metadata": {
        "id": "CK5h1e3_2OKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_video('cartpole-agent/dqn-test-bestparam-episode-3.mp4')"
      ],
      "metadata": {
        "id": "h5Jn3BaZ5Ovd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_as_gif(\"cartpole-agent/dqn-test-bestparam-episode-3.mp4\")"
      ],
      "metadata": {
        "id": "2bf1Prtl6KJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Double DQN"
      ],
      "metadata": {
        "id": "u5649jFv-82y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yine benzer mantıkla bunu da DQNAgent'tan inherit edeceğiz ve sadece do_calculation metodunu override edeceğiz."
      ],
      "metadata": {
        "id": "3gH4R9yhg2T0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DoubleDQNAgent(DQNAgent):\n",
        "    #sadece do_calculation'ı override etmemiz yeterli\n",
        "    def do_calculation(self):\n",
        "        # 1) Target Q’ları hesapla\n",
        "        q_next_target = self.target_model.predict(self.next_states, verbose=0)\n",
        "        q_online = self.online_model.predict(self.next_states, verbose=0) #aksiyon seçimi için online ağdan tahmin alıyoruz\n",
        "        next_actions = np.argmax(q_online, axis=1)\n",
        "        max_next_q = q_next_target[np.arange(self.batch_size), next_actions] #online ağdan en büyük aksiyon seçilir\n",
        "        q_targets = self.rewards + self.gamma * max_next_q * (1 - self.dones)\n",
        "        if self.verbose:\n",
        "          print(f\"q_next_target:{q_next_target}\")\n",
        "\n",
        "        self.continue_calculation(q_targets)"
      ],
      "metadata": {
        "id": "5wCh4g0ML330"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"CartPole-v1\")\n",
        "double_dqnagent = DoubleDQNAgent(env,batch_size=32, learning_rate= 0.005, memory_size= 2000, target_update=5) #hyperparameter tuningden dönen en iyi değerleri kullanalım,\n",
        "# agent.verbose = True #Uncomment yaparak repaly metodu içindeki print'li kısımların görünmesini sağlayabilirsiniz\n",
        "printInsideTraining = False # T/F yaparak bu hücre içindeki printlerin görünüp görünmemesini sağlayabilirsiniz\n",
        "# kodda kalabalık odluğunu düşünüyorsanız ve print'lere gerek duymuyorsanız o satırları silebilirsiniz\n",
        "\n",
        "train_model(env, double_dqnagent, episodes=200, print_episode=True) #ilk 100 civarında çok hızlı ilerledi, 15 dk gibi, sonra hızlı öğrenmeye başladı ve eğitim süresi uzadı"
      ],
      "metadata": {
        "id": "h5ZBUfLRs1fd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pure tensorflow yaklaşımı"
      ],
      "metadata": {
        "id": "o0nCwc7NKOZM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bizim yazdığımız kodda replay metodu numpy ile vektörize hale getirilerek hızlandırılmıştı. Bunun daha da hızlı yolu tensorlarla çalışmak. Bunun için pure python ile yazılmış gymnasium environment'ının tensorflowcasını kullanmak gerekiyor, yani gymanisum environmentları uygun tf wrapperlarla tensorflowlaştırılıyor.\n",
        "\n",
        "Bunu biz burada kodlamak yerine işi üstadlara bırakalım, [buradan](https://www.tensorflow.org/agents/tutorials/1_dqn_tutorial) erişebilirsiniz."
      ],
      "metadata": {
        "id": "G9_TBv8QKlEn"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "toc_visible": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPRWVoZW8tyHFtb4iLuLPkJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}